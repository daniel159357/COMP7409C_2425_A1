import numpy as np 

class Perceptron():
  def __init__(self, eta=0.01, nepoch=10, random_state=1): 
    '''Meaning of the parameters:
    eta: learning rate (a small float number between 0.0 and 1.0) 
    nepoch: No of passes over the whole training dataset 
    random_state: for fixing the initial state of the generator for random numbers 
    so that yours will be the same as those generated by our TAs' program.
    '''
    #   insert statements below for setting self.eta, self.nepoch and self.random_state by the arguments of the function.
    self.eta = eta
    self.nepoch = nepoch
    self.random_state = random_state
    
  def fit(self, X, y):
    ''' comments
    meaning of the parameters:
    X: traning data, which is a 2D array with shape (n_examples, n_features) where n_examples is the number of examples 
       and n_features is the number of features
    y: labels, which is a 1D array with shape (n_examples)
    '''
    # the following statements are for initializing the weights by some small random numbers.
    # rgen is the random number generator with its random_state fixed at the one you given when you create the object by Preceptron()
    rgen = np.random.RandomState(seed = self.random_state)
    self.w = rgen.normal(loc=0.0, scale=0.01, size = 1 + X.shape[1]) #to generate a random number for each weight from the normal distribution with mean = 0 and std = 0.01.
    self.errors = [] # the list self.errors is for recording the total number of errors made in each epoch.
    
    # the following for loop processes the whole data set for weight updates for self.nepoch times
    for _ in range(self.nepoch):
      errors = 0
      # insert necessary statements below to read in each (example, label) pair and 
      # update the weights using the Perceptron learning rules given in the lecture.
      # ...
      for xi, yi in zip(X, y):
        # calculate the net input
        net_input = self.net_input(xi)
        # calculate the output
        pred = np.where(net_input >= 0.0, 1, -1)
        # calculate the error
        error = yi - pred
        # update the weights
        self.w[0] += self.eta * error
        self.w[1:] += self.eta * error * xi
        errors += error
        # end of loop

    # insert necessary statements below to append the number of errors found in this round to self.errors here
    self.errors.append(errors)
    return self
  
  def net_input(self, x):
    ''' comments
    As can be seen in the function fit(), the weights are stored in
    the array self.w. For the argument x, it represents
    an example with feature x_1, x_2, ..., x_n and
    the function returns the net input
    self.w_0 + self.w1*x_1 + self.w2*x_2 + ... + self.w_n *xn
    '''
    # insert statements below to complete the function definition.
    return np.dot(x, self.w[1:]) + self.w[0]
    
  def predict(self, X):
    '''comments
    return the class labels for the data set X based
    on their net inputs.
    '''
    # insert statements below to complete the function definition.
    preds = []
    for xi in X:
      net_input = self.net_input(xi)
      pred = np.where(net_input >= 0.0, 1, -1)
      preds.append(pred)
    return np.array(preds)
